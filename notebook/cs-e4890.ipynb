{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-E4890 – Jigsaw Unintended Bias in Toxicity Classification\n",
    "\n",
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of this project is to attempt to solve a toxicity classification problem in the field of natural language processing (NLP). The approach used in this paper discusses neural networks and how they perform for this classification task. We discuss our reasoning, expectations and thoughts of the execution of our project. The toxicity classification problem at hand is due to the increasing amount of discussion platforms such as, comment feeds on live broadcasts and other forums. Some live broadcasts are targeted for wide audiences and need efficient, near instantanious, profanity filtering.\n",
    "\n",
    "Typically, the issue with naïve profanity filtering is focusing too heavily on individual words and ignoring the context. It was studied that names of the frequently attacked identities were automatically accociated to toxicity by machine learning (ML) models, even if the individual(s) themselves, or the context, were not offensive. (Kaggle. 2019) \n",
    "\n",
    "In this paper, the overall assessment of the goodness of our model is deviced with an overall Area under the curve (AUC) Receiver operating characteristic curve (ROC) test and with multiple other submetrics. The submetrics are:\n",
    "- Bias AUCs: To prevent unintended bias we use three specific subsets of the test for each identity, attemping to capture all the aspects of unintended bias.\n",
    "     - Subgroup AUC: Focuses on specific identity subgroup\n",
    "     - Background Positive, Subgroup negative (BSPSN) AUC: Evaluates only non-toxic examples for an identity and toxic examples without the identity\n",
    "     - Background Negative, Subgroup positive (BNSP) AUC: The test set is restricted opposite to BSPN, only featuring examples of toxic examples of an identity and non-toxic examples without the identity\n",
    "- Generalized Mean of Bias AUCs\n",
    "    - Calculates the combination of the per-identity Bias AUCs into one overall measure with:\n",
    "$$M_p(m_s) = \\left(\\frac{1}{N} \\sum_{s=1}^{N} m_s^p\\right)^\\frac{1}{p}$$\n",
    "where:\n",
    "- $M_p$ = the pth power-mean function\n",
    "- $m_s$ = the bias metric m calulated for subgroup s\n",
    "- $N$ = number of identity subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, after obtaining the overall AUC and the General Mean of Bias AUCs the calculation of the final model is done with formula:\n",
    "$$score = w_0 AUC_{overall} + \\sum_{a=1}^{A} w_a M_p(m_{s,a})$$\n",
    "where:\n",
    "- $A$ = number of submetrics (3)\n",
    "- $m_s,a$ = bias metric for identity subgroup $s$ using submetric $a$\n",
    "- $w_a$ = a weighting for the relative importance of each submetric; all four $w$ values set to 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "import re # regexes\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import os\n",
    "\n",
    "import time # timestamps\n",
    "import gc\n",
    "import random\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the datasets\n",
    "The origin of the data is from year 2017 when the Civil Comments platform shut down and published their ~2m public comments making them available for researchers. Each item in the dataset  has one primary attribute for the toxicity, `target`, indicating a goal value that the models should try to achieve. The trained model should then predict the `target` toxicity for the test data.\n",
    "\n",
    "In addition to `target`, there are several subtypes of toxicity. These are not supposed to be predicted by the model, but they are for providing additional avenue for future research. The subtype attributes are:\n",
    "- severe_toxicity\n",
    "- obscene\n",
    "- threat\n",
    "- insult\n",
    "- identity_attack\n",
    "- sexual_explicit\n",
    "\n",
    "Along with these, we may use attribute `parent_id` for training our model. The reason for this is that we think that the neural network should mark some difference between comments that start a thread versus ones that do not.\n",
    "\n",
    "Some of the comments have a label for identity. There are multiple identity attributes, each representing the identity that is mentioned in the comment. The identities we are interested, as the ones used in the validation of the model, are:\n",
    "- male\n",
    "- female\n",
    "- homosexuality_gay_or_lesbian\n",
    "- christian\n",
    "- jewish\n",
    "- muslim\n",
    "- black\n",
    "- white\n",
    "- psychiatric_or_mental_illness\n",
    "\n",
    "Based on these, we decided to select the following columns for training the model and drop all other columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_columns = [\n",
    "    \"male\",\n",
    "    \"female\",\n",
    "    \"homosexual_gay_or_lesbian\",\n",
    "    \"christian\",\n",
    "    \"jewish\",\n",
    "    \"muslim\",\n",
    "    \"black\",\n",
    "    \"white\",\n",
    "    \"psychiatric_or_mental_illness\"\n",
    "]\n",
    "\n",
    "relevant_columns = [\n",
    "    \"id\",\n",
    "    \"target\",\n",
    "    \"comment_text\"\n",
    "] + identity_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the data\n",
    "\n",
    "The dataset contains duplicate comments with same content. However, the different comments may have been labelled with different targets or subgroups [kaggle].\n",
    "\n",
    "The operations we do for the datasets are; lowercase the words, remove the non-alpha characters, fill empty values with 0 and preprocess the data so that we obtain a much smaller, more compact, sets for the training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = None\n",
    "x_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cleaned_file(from_name, to_name, cols, drop_duplicates):\n",
    "    \"\"\"Create a cleaned file from a file\"\"\"\n",
    "    data = pd.read_csv(\n",
    "        f\"../input/{from_name}.csv\",\n",
    "        usecols=cols, # use only relevant columns, as specified before in the notebook\n",
    "    )\n",
    "    data.set_index(\"id\", inplace=True)\n",
    "\n",
    "    words = set()\n",
    "    data[\"comment_text\"].str.split().apply(words.update)\n",
    "\n",
    "    # Remove all non-alphanumeric or space characters from comment text\n",
    "    data[\"comment_text\"] = data[\"comment_text\"].transform(lambda s: re.sub(\"[^a-z\\d\\s]\", \" \", s.lower()))\n",
    "    data = data.fillna(0) # fill empty values with 0\n",
    "\n",
    "    if drop_duplicates:\n",
    "        cleaned_words = set()\n",
    "        data[\"comment_text\"].str.split().apply(cleaned_words.update)\n",
    "\n",
    "        # Write summary file for visualising the cleanup process\n",
    "        pd.DataFrame({\n",
    "            \"previous_word_count\": [len(words)],\n",
    "            \"cleaned_word_count\": [len(cleaned_words)],\n",
    "            \"previous_row_count\": [len(data)],\n",
    "            \"cleaned_row_count\": [data['comment_text'].nunique()]\n",
    "        }).to_csv(\"./\"+to_name+\"_summary.csv\")\n",
    "\n",
    "        data = data.groupby('comment_text').mean().reset_index()\n",
    "    data.to_csv(\"./\"+to_name+\".csv\")\n",
    "\n",
    "def print_cleanup_summary(filename):\n",
    "    # read cleanup summary from saved file\n",
    "    cleanup_summary = pd.read_csv(\"./\"+filename+\"_summary.csv\")\n",
    "    initial_row_count, initial_word_count = cleanup_summary.loc[\n",
    "        0,\n",
    "        [\"previous_row_count\", \"previous_word_count\"]\n",
    "    ]\n",
    "    cleaned_row_count, cleaned_word_count = cleanup_summary.loc[\n",
    "        cleanup_summary.index[-1],\n",
    "        [\"cleaned_row_count\", \"cleaned_word_count\"]\n",
    "    ]\n",
    "\n",
    "    print(\"The original data was reducted by {} rows ({:.2f}%) and by {} words ({:.2f}%)\".format( \n",
    "        initial_row_count - cleaned_row_count,\n",
    "        100 * (1 - cleaned_row_count / initial_row_count),\n",
    "        initial_word_count - cleaned_word_count,\n",
    "        100 * (1 - cleaned_word_count / initial_word_count)\n",
    "    ))\n",
    "    \n",
    "def read_cleaned_file(data, from_name, to_name, cols, drop_duplicates=True):\n",
    "    if not os.path.isfile(\"./\"+to_name+\".csv\"):\n",
    "        create_cleaned_file(from_name, to_name, cols, drop_duplicates)\n",
    "    # read data from cleaned data file if not already set\n",
    "    if data is None:\n",
    "        data = pd.read_csv(\"./\"+to_name+\".csv\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = read_cleaned_file(x_train, 'train', 'train_cleaned', relevant_columns)\n",
    "y_train = np.where(x_train['target'] >= .5, 1, 0)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "x_test = read_cleaned_file(x_test, 'test', 'test_cleaned', [\"id\", \"comment_text\"], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cleanup_summary(\"train_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.sort_values(by=[\"jewish\", \"target\"], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Methods\n",
    "\n",
    "Natural language processing (NLP) is an example of a Supervised Machine Learning task that focuses in labelled datasets containing sequences and or single words. The purpose of NLP is to train a classifier that can distinguish the sets of words into their right belonging categories (classes).\n",
    "\n",
    "Typically the text classification pipeline contains the following components:\n",
    "- Training data: Input text for the model\n",
    "- Feature vector: A vector that describe the input texts, in some charasteristic or multiple ones\n",
    "- Labels: The classes that we plan to predict with our trained model\n",
    "- Algorithm: A machine learning algorithm that is used to classify the inputs\n",
    "- Model: The result of the training, this will perform the label predictions ( https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f )\n",
    "\n",
    "A prime example of a text classification problem is detection of spam or toxic sentences.\n",
    "\n",
    "Use of recursive neural networks (RNN) in natural language processing (NLP) is an intuitive and an advisable method. (**SOURCE**) RNNs exceed in grammar learning as the order of words may have an effect on the context of the sentence. This is mainly because their ability to process sequences of inputs, such as, in the case of toxicity filtering, sequences of words.\n",
    "\n",
    "### Convolutional Neural Network (CNN)\n",
    "\n",
    "CNNs are feed-forward artificial neural networks. They use a variation of multilayer perceptrons that offer minimal preprocessing. Use of CNNs in NLP is a relatively new technique as previously their primary use case was in computer vision. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive neural network (RNN)\n",
    "\n",
    "In RNN the nodes of the network form a directed graph along with a temporal sequence. Each individual input of the sequence is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "This section is for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "CRAWL_EMBEDDING_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING_PATH = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "NUM_MODELS = 2\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Train\n",
    "def train_model(model, train, test, loss_fn, output_dim, lr=0.001,\n",
    "                batch_size=512, n_epochs=4,\n",
    "                enable_checkpoint_ensemble=True):\n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "\n",
    "            y_pred = model(*x_batch)            \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        model.eval()\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "    \n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy())\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n",
    "\n",
    "        all_test_preds.append(test_preds)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "              epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "\n",
    "    if enable_checkpoint_ensemble:\n",
    "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
    "    else:\n",
    "        test_preds = all_test_preds[-1]\n",
    "        \n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "\n",
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_aux_targets):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    \n",
    "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = max_features or len(tokenizer.word_index) + 1\n",
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
    "print('n unknown words (crawl): ', len(unknown_words_crawl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n",
    "print('n unknown words (glove): ', len(unknown_words_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "embedding_matrix.shape\n",
    "\n",
    "del crawl_matrix\n",
    "del glove_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "x_train_torch = torch.tensor(x_train, dtype=torch.long)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "print(x_train_torch.size())\n",
    "print(y_train_torch.size())\n",
    "\n",
    "x_test_torch = torch.tensor(x_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.TensorDataset(x_train_torch, y_train_torch)\n",
    "test_dataset = data.TensorDataset(x_test_torch)\n",
    "\n",
    "all_test_preds = []\n",
    "\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    print('Model ', model_idx)\n",
    "    seed_everything(1234 + model_idx)\n",
    "    \n",
    "    model = NeuralNet(embedding_matrix, 10)\n",
    "    \n",
    "    test_preds = train_model(model, train_dataset, test_dataset, output_dim=10, \n",
    "                             loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n",
    "    all_test_preds.append(test_preds)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-directional LSTM\n",
    "\n",
    "foo bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Yenala, H., Jhanwar, A., Chinnakotla, M.K. et al. Int J Data Sci Anal (2018) 6: 273. https://doi.org/10.1007/s41060-017-0088-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitch\n",
    "\n",
    "### 3) Write below a short description of the machine learning problem you plan to address\n",
    "\n",
    "Detecting toxic comments while minimizing uninted model bias. Jigsaw Unintended Bias in Toxicity Classification.\n",
    "<https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview>.\n",
    "\n",
    "### 4) Write below what deep learning approach(es) you plan to employ in your project\n",
    "\n",
    "At first we will inspect the data and manipulate it by different methods (e.g. converting that's -> that is). After analyzing the data, we'll feed it to neural network.\n",
    "We plan on using Convolution Neural Networks (CNN) because they have been shown effective for various natural language processing (NLP) problems.\n",
    "\n",
    "In addition to CNNs, we will implement a Long short-term memory (LSTM) recurrent neural network (RNN) or a GRU network (a variation of LSTM),\n",
    "and compare those results to the results obtained by CNNs.\n",
    "\n",
    "###  5) Write below what deep learning software you plan to use in your project\n",
    "\n",
    "We will use PyTorch.\n",
    "\n",
    "### 6) Write below what computational resources you plan to utilize in your project\n",
    "\n",
    "We use Kaggle's computational cloud environment, Kaggle Kernels.\n",
    "\n",
    "At time of writing, each Kernel editing session is provided with the following resources:\n",
    "- 9 hours execution time\n",
    "- 5 Gigabytes of auto-saved disk space (/kaggle/working)\n",
    "- 16 Gigabytes of temporary, scratchpad disk space (outside /kaggle/working)\n",
    "\n",
    "CPU Specifications\n",
    "- 4 CPU cores\n",
    "- 17 Gigabytes of RAM\n",
    "\n",
    "GPU Specifications\n",
    "- 2 CPU cores\n",
    "- 14 Gigabytes of RAM\n",
    "\n",
    "### 7) Write below what kind of data you plan to use in your experiments\n",
    "\n",
    "Data provided in the Kaggle competition (format of files is .csv).\n",
    "\n",
    "### 8) Write below what are the reference methods and results you plan to compare against\n",
    "\n",
    "Overall AUC:\n",
    "- This is the ROC-AUC (Receiver operating characteristic curve - Area Under the ROC Curve) for the full evaluation set.\n",
    "\n",
    "Bis AUCs:\n",
    "- To measure unintended bias, we again calculate the ROC-AUC, this time on three specific subsets of the test set for each identity, each capturing a different aspect of unintended bias.\n",
    "  - Subgroup AUC\n",
    "  - BPSN\n",
    "  - BNSP\n",
    "\n",
    "Generalized Mean of Bias AUCs:\n",
    "- To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean.\n",
    "\n",
    "Final Metric:\n",
    "- We combine the overall AUC with the generalized mean of the Bias AUCs to calculate the final model score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "### Section 1. Problem and data description (3 pts)\n",
    "\n",
    "#### 1.1. The report should describe the problem and/or research questions addressed in the project. (1 pt)\n",
    "\n",
    "* It is unclear what problem the project tries to solve.\n",
    "* The problem is described but some details are missing.\n",
    "* The problem is described well.\n",
    "\n",
    "#### 1.2. Bonus: Is the problem novel and/or original? (1 pt)\n",
    "\n",
    "* No\n",
    "* Yes\n",
    "\n",
    "#### 1.3 Data description. (1 pt)\n",
    "\n",
    "Describe data dimensionalities, number of training samples, the format used.\n",
    "\n",
    "* The data is not described.\n",
    "* The data is described but some details are missing.\n",
    "* The data is described well.\n",
    "\n",
    "#### 1.4. Please describe what details were missing in the problem/data description.\n",
    "\n",
    "### Section 2. Method (6 pts)\n",
    "\n",
    "#### 2.1. Method description. (2 pts)\n",
    "\n",
    "The report should describe well the model used in the project. If the model was covered in the lectures, it is ok to describe the architecture (such as, e.g., the number of layers etc) without going into details (such as computations in a basic convolutional layer). If the model was not covered in the lectures, you need to provide enough details so that your classmates can understand it without checking external references.\n",
    "\n",
    "* The model is not described.\n",
    "* The model is described well but some details are missing.\n",
    "* The model is described very well. I could implement the model based on the description.\n",
    "\n",
    "#### 2.2. Choice of the model. (2 pts)\n",
    "\n",
    "* The proposed model is not reasonable for the task.\n",
    "* The model is reasonable but some choices are questionable.\n",
    "* The model is suitable for the task.\n",
    "\n",
    "#### 2.3. Bonus: Is the model novel and/or original? (2 pts)\n",
    "\n",
    "* No\n",
    "* Partly\n",
    "* Yes, the model deserves to be presented in a conference\n",
    "\n",
    "#### 2.4. If you think that the model is not perfectly suitable for the task, please write your suggestions on how the model could be improved.\n",
    "\n",
    "### Section 3. Experiments and results (4 pts)\n",
    "\n",
    "#### 3.1. Are the experiments described well in the report? (2 pts)\n",
    "\n",
    "* The experiments are not described.\n",
    "* Experiments are described but some details are missing.\n",
    "* Experiments are well described. I could reproduce the experiments based on the description.\n",
    "\n",
    "#### 3.2. Performance of the proposed model (2 pts)\n",
    "\n",
    "* It is difficult to evaluate the performance (there is no baseline or no demo for tasks that require subjective evaluation).\n",
    "* The results are adequate.\n",
    "* The results are impressive (either close to the state of the art or good subjective evaluation).\n",
    "\n",
    "### Section 4. Conclusions (1 pt)\n",
    "\n",
    "#### 4.1. Conclusions are adequate:\n",
    "* No\n",
    "* Yes\n",
    "\n",
    "#### 4.2. Optional feedback on the conclusions.\n",
    "\n",
    "### Section 5. Evaluate the code. (3 pts)\n",
    "\n",
    "### Section 6. Overall feedback (3 pts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
